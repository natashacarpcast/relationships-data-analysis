{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook applies VADER Sentiment Analysis to each submission from the already processed reddit dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.pyspark.python': 'python3', 'spark.pyspark.virtualenv.enabled': 'true', 'spark.pyspark.virtualenv.type': 'native', 'spark.pyspark.virtualenv.bin.path': '/usr/bin/virtualenv'}, 'proxyUser': 'jovyan', 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "No active sessions."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\": {\n",
    "        \"spark.pyspark.python\": \"python3\",\n",
    "        \"spark.pyspark.virtualenv.enabled\": \"true\",\n",
    "        \"spark.pyspark.virtualenv.type\":\"native\",\n",
    "        \"spark.pyspark.virtualenv.bin.path\":\"/usr/bin/virtualenv\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>8</td><td>application_1716476646286_0010</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-44-186.ec2.internal:20888/proxy/application_1716476646286_0010/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-45-192.ec2.internal:8042/node/containerlogs/container_1716476646286_0010_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting vaderSentiment\n",
      "  Using cached https://files.pythonhosted.org/packages/76/fc/310e16254683c1ed35eeb97386986d6c00bc29df17ce280aed64d55537e9/vaderSentiment-3.3.2-py2.py3-none-any.whl\n",
      "Collecting requests (from vaderSentiment)\n",
      "  Using cached https://files.pythonhosted.org/packages/70/8e/0e2d847013cb52cd35b38c009bb167a1a26b2ce6cd6965bf26b47bc0bf44/requests-2.31.0-py3-none-any.whl\n",
      "Collecting charset-normalizer<4,>=2 (from requests->vaderSentiment)\n",
      "  Using cached https://files.pythonhosted.org/packages/28/76/e6222113b83e3622caa4bb41032d0b1bf785250607392e1b778aca0b8a7d/charset_normalizer-3.3.2-py3-none-any.whl\n",
      "Collecting certifi>=2017.4.17 (from requests->vaderSentiment)\n",
      "  Using cached https://files.pythonhosted.org/packages/ba/06/a07f096c664aeb9f01624f858c3add0a4e913d6c96257acb4fce61e7de14/certifi-2024.2.2-py3-none-any.whl\n",
      "Collecting idna<4,>=2.5 (from requests->vaderSentiment)\n",
      "  Using cached https://files.pythonhosted.org/packages/e5/3e/741d8c82801c347547f8a2a06aa57dbb1992be9e948df2ea0eda2c8b79e8/idna-3.7-py3-none-any.whl\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->vaderSentiment)\n",
      "  Using cached https://files.pythonhosted.org/packages/d2/b2/b157855192a68541a91ba7b2bbcb91f1b4faa51f8bae38d8005c034be524/urllib3-2.0.7-py3-none-any.whl\n",
      "Installing collected packages: charset-normalizer, certifi, idna, urllib3, requests, vaderSentiment\n",
      "Successfully installed certifi-2024.2.2 charset-normalizer-3.3.2 idna-3.7 requests-2.31.0 urllib3-2.0.7 vaderSentiment-3.3.2\n",
      "\n",
      "Collecting pandas==1.0.5\n",
      "  Using cached https://files.pythonhosted.org/packages/af/f3/683bf2547a3eaeec15b39cef86f61e921b3b187f250fcd2b5c5fb4386369/pandas-1.0.5-cp37-cp37m-manylinux1_x86_64.whl\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib64/python3.7/site-packages (from pandas==1.0.5)\n",
      "Collecting python-dateutil>=2.6.1 (from pandas==1.0.5)\n",
      "  Using cached https://files.pythonhosted.org/packages/ec/57/56b9bcc3c9c6a792fcbaf139543cee77261f3651ca9da0c93f5c1221264b/python_dateutil-2.9.0.post0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/site-packages (from pandas==1.0.5)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas==1.0.5)\n",
      "Installing collected packages: python-dateutil, pandas\n",
      "Successfully installed pandas-1.0.5 python-dateutil-2.9.0.post0\n",
      "\n",
      "Collecting pyarrow==0.15.1\n",
      "  Downloading https://files.pythonhosted.org/packages/2c/08/c3edfb14bf58af6d9b1d50e555232d82276ea9bbe4cb88dd491f137bc8d0/pyarrow-0.15.1-cp37-cp37m-manylinux1_x86_64.whl (59.0MB)\n",
      "Requirement already satisfied: numpy>=1.14 in /usr/local/lib64/python3.7/site-packages (from pyarrow==0.15.1)\n",
      "Requirement already satisfied: six>=1.0.0 in /usr/local/lib/python3.7/site-packages (from pyarrow==0.15.1)\n",
      "Installing collected packages: pyarrow\n",
      "Successfully installed pyarrow-0.15.1"
     ]
    }
   ],
   "source": [
    "sc.install_pypi_package(\"vaderSentiment\",\"https://pypi.org/simple\")\n",
    "sc.install_pypi_package(\"pandas==1.0.5\")\n",
    "sc.install_pypi_package(\"pyarrow==0.15.1\", \"https://pypi.org/simple\")\n",
    "\n",
    "#Since I needed a nuanced score of sentiment and not just labels, I'm using \n",
    "#VADER instead of the Spark NLP built-in functions \n",
    "\n",
    "#Pandas and Pyarrow packages will be used for creating Pandas UDF \n",
    "\n",
    "#https://github.com/cjhutto/vaderSentiment?tab=readme-ov-file#code-examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = spark.read.parquet(\"s3://finalproject-nat-s3/data_withtopic_labels/*.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157"
     ]
    }
   ],
   "source": [
    "df.filter(F.col(\"entire_text\").isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#I'm seeing that some rows were not correctly cleaned up before, so I'll remove those rows \n",
    "#(I would clean up again the original file but the topic modeling was too computationally expensive to run again)\n",
    "df_filtered = df.filter(F.col(\"entire_text\").isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0"
     ]
    }
   ],
   "source": [
    "#Ensure they got deleted\n",
    "df_filtered.filter(F.col(\"entire_text\").isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/functions.py:386: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details."
     ]
    }
   ],
   "source": [
    "#Create a user defined function that gets the levels of negativity. \n",
    "\n",
    "#Since the subreddit is mainly\n",
    "#for discussing problems, I do not expect positive texts. Instead, I want to \n",
    "#capture the nuances of negativity.\n",
    "\n",
    "#Which topics are discussed with more or less intensity of negativity?\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer() \n",
    "\n",
    "#Use a pandas udf to optimize applying the function to all rows \n",
    "#https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.pandas_udf.html\n",
    "\n",
    "@pandas_udf('float', PandasUDFType.SCALAR)\n",
    "def get_negativity_score_udf(text: pd.Series) -> pd.Series:\n",
    "    return text.apply(lambda x: analyzer.polarity_scores(x)['neg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'18971M'"
     ]
    }
   ],
   "source": [
    "#Ensure executers are using enough memory for persisting the dataframe\n",
    "sc._conf.get('spark.executor.memory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Persist DataFrame and apply UDF\n",
    "df_filtered.persist()\n",
    "df_w_neg_scores = df_filtered.withColumn('negativity_score', get_negativity_score_udf(F.col('entire_text')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+------------+--------------------+----+--------------------+----------------+\n",
      "|     id|score|num_comments|         entire_text|year|         topic_label|negativity_score|\n",
      "+-------+-----+------------+--------------------+----+--------------------+----------------+\n",
      "| acnfj5|   24|          30|is it reasonable ...|2019|       Unclear topic|           0.054|\n",
      "| emdrc4|    3|           8|im now my ex and ...|2020|       Unclear topic|           0.092|\n",
      "|15gdohy|    1|           3|my 44m boyfriend ...|2023|       Unclear topic|           0.089|\n",
      "|161n6ru|    1|           1|how do i 18f cont...|2023|Romantic relation...|           0.114|\n",
      "| cyp5td|    2|           7|how do i get peop...|2019|       Unclear topic|           0.078|\n",
      "+-------+-----+------------+--------------------+----+--------------------+----------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "df_w_neg_scores.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save scores into S3 for later visualization\n",
    "df_repartitioned = df_w_neg_scores.repartition(10)\n",
    "df_repartitioned.write.parquet(\"s3://finalproject-nat-s3/final_data\", mode = 'overwrite')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

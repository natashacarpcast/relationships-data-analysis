{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook does an initial preprocessing from the raw data obtained from AcademicTorrents. Then, it uploads the processed data into an S3 bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Spark was not reading the CSV correctly due to some punctuation characters and entries divided among multiple lines, it had to be preprocessed locally before uploading it to S3 for further cleaning and analysis. Although perhaps slow (~3 min), it was an important step. \n",
    "\n",
    "Use of AI - Part of this code was created with ChatGPT with the prompt \"I have a huge dataset in a csv and some entries have different rows, how can I do in pyspark so it recognizes an entire cell as just one row?\".\n",
    "\n",
    "Then, it was modified to also delete punctuation and facilitate parsing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing complete. Processed data saved\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import string \n",
    "\n",
    "with open('submissions.csv', 'r', newline='') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    with open('processed_submissions.csv', 'w', newline='') as outfile:\n",
    "        writer = csv.DictWriter(outfile, fieldnames=reader.fieldnames)\n",
    "        writer.writeheader()\n",
    "        for row in reader:\n",
    "            # Concatenate multiline fields\n",
    "            row['selftext'] = ' '.join(row['selftext'].splitlines())\n",
    "            row['title'] = ' '.join(row['title'].splitlines())\n",
    "\n",
    "            # Remove punctuation \n",
    "            to_remove = string.punctuation + \"â€™\" #Add additional character to\n",
    "                                                #default punctuation characters\n",
    "            translator = str.maketrans('', '', to_remove) \n",
    "            row['selftext'] = row['selftext'].translate(translator) \n",
    "            row['title'] = row['title'].translate(translator) \n",
    "\n",
    "            # Write the processed row to the output file\n",
    "            writer.writerow(row)\n",
    "\n",
    "print(\"Preprocessing complete. Processed data saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "s3_resource = boto3.resource('s3')\n",
    "\n",
    "iam_client = boto3.client('iam')\n",
    "role = iam_client.get_role(RoleName='LabRole')\n",
    "\n",
    "s3.upload_file(Filename='processed_submissions.csv', \n",
    "               Bucket='finalproject-nat-s3',\n",
    "               Key = 'processed_submissions.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "macs30112",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
